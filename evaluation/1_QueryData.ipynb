{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Imports"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from google.oauth2 import service_account\n",
    "from google.cloud import monitoring_v3\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud.bigquery.job import ExtractJobConfig\n",
    "import time\n",
    "import matplotlib.dates as mdates\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import datetime\n",
    "import sharedVariables\n",
    "from sharedVariables import *"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "todaystring = datetime.datetime.today().strftime('%Y%m%d')\n",
    "outdir = f\"data/{todaystring}\"\n",
    "if not os.path.exists(outdir):\n",
    "    os.mkdir(outdir)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Network Logs"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    '../infrastructure/credentials.json')\n",
    "\n",
    "client = bigquery.Client(credentials=credentials)\n",
    "today = datetime.datetime.today()\n",
    "table_id = f\"compute_googleapis_com_vpc_flows_{today.strftime('%Y%m%d')}\"\n",
    "# table_id = 'compute_googleapis_com_vpc_flows_20210720'\n",
    "blob_name = \"export.log\"\n",
    "\n",
    "\n",
    "\n",
    "extract_conf = ExtractJobConfig()\n",
    "extract_conf.compression = 'NONE'\n",
    "extract_conf.destination_format = 'NEWLINE_DELIMITED_JSON'\n",
    "\n",
    "def getDataset(experiment): \n",
    "    bucket_name = experiment + \"-log-bucket\"\n",
    "\n",
    "    destination_uri = \"gs://{}/{}\".format(bucket_name, blob_name)\n",
    "    dataset_ref = bigquery.DatasetReference(project, experiment.replace(\"-\", \"_\"))\n",
    "    table_ref = dataset_ref.table(table_id)\n",
    "\n",
    "    extract_job = client.extract_table(\n",
    "        table_ref,\n",
    "        destination_uri,\n",
    "        # Location must match that of the source table.\n",
    "        location=\"US\",\n",
    "        job_config=extract_conf\n",
    "    )  # API request\n",
    "    extract_job.result()  # Waits for job to complete.\n",
    "\n",
    "    print(\n",
    "        \"Exported {}:{}.{} to {}\".format(project, experiment, table_id, destination_uri)\n",
    "    )\n",
    "\n",
    "\n",
    "    storage_client = storage.Client(credentials=credentials)\n",
    "\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "    blob.download_to_filename(f\"{outdir}/{experiment}.log\")\n",
    "\n",
    "    print(\n",
    "        \"Blob downloaded successfully.\"\n",
    "    )\n",
    "\n",
    "for experiment in sharedVariables.experiments: \n",
    "    try: \n",
    "        getDataset(experiment)\n",
    "    except Exception as e: \n",
    "        print(e)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Exported dspj-315716:experiment-baseline-with-latency.compute_googleapis_com_vpc_flows_20210720 to gs://experiment-baseline-with-latency-log-bucket/export.log\n",
      "Blob downloaded successfully.\n",
      "Exported dspj-315716:experiment-syncmesh-with-latency.compute_googleapis_com_vpc_flows_20210720 to gs://experiment-syncmesh-with-latency-log-bucket/export.log\n",
      "Blob downloaded successfully.\n",
      "Exported dspj-315716:experiment-syncmesh-with-latency-6.compute_googleapis_com_vpc_flows_20210720 to gs://experiment-syncmesh-with-latency-6-log-bucket/export.log\n",
      "Blob downloaded successfully.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Get TimeStamps"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "df_sync = loadData(f'{outdir}/experiment-syncmesh-with-latency.log')\n",
    "df_base = loadData(f'{outdir}/experiment-baseline-with-latency.log')\n",
    "\n",
    "def filterDataForSeperator(df): \n",
    "    df = df[df[\"jsonPayload.connection.src_ip\"].isin([ip_seperator, ip_orchestrator])]\n",
    "    df = df[df[\"jsonPayload.connection.dest_ip\"].isin([ip_seperator, ip_orchestrator])]\n",
    "    # df = df[df[\"jsonPayload.connection.dest_port\"] == 443]\n",
    "    return df\n",
    "\n",
    "seperator_base = filterDataForSeperator(df_base)\n",
    "seperator_sync = filterDataForSeperator(df_sync)\n",
    "\n",
    "# df = df[df[\"jsonPayload.connection.dest_ip\"]]\n",
    "print(seperator_sync.index)\n",
    "print(seperator_base.index)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "DatetimeIndex(['2021-07-20 23:38:00.580331+00:00', '2021-07-20 23:38:00.580331+00:00'], dtype='datetime64[ns, UTC]', name='timestamp', freq=None)\n",
      "DatetimeIndex(['2021-07-20 23:27:26.878197+00:00',\n",
      "               '2021-07-20 23:27:26.878197+00:00',\n",
      "               '2021-07-20 23:31:27.925317+00:00',\n",
      "               '2021-07-20 23:31:27.925317+00:00'],\n",
      "              dtype='datetime64[ns, UTC]', name='timestamp', freq=None)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Monitoring"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    '../infrastructure/credentials.json')\n",
    "\n",
    "# Our project ID\n",
    "\n",
    "client = monitoring_v3.MetricServiceClient(credentials=credentials)\n",
    "project_name = f\"projects/{project_id}\"\n",
    "now = time.time()\n",
    "seconds = int(now)\n",
    "nanos = int((now - seconds) * 10 ** 9)\n",
    "interval = monitoring_v3.TimeInterval(\n",
    "    {\n",
    "        \"end_time\": {\"seconds\": seconds, \"nanos\": nanos},\n",
    "        # 3600 = Get the last hour of metrics\n",
    "        \"start_time\": {\"seconds\": (seconds - (3600 * 1)), \"nanos\": nanos},\n",
    "    }\n",
    ")\n",
    "\n",
    "# Add Filter? metric.label.instance_name = \"your-instance-id\"\n",
    "results_cpu = client.list_time_series(\n",
    "    request={\n",
    "        \"name\": project_name,\n",
    "        \"filter\": 'metric.type =  \"compute.googleapis.com/instance/cpu/utilization\"',\n",
    "        \"interval\": interval,\n",
    "        \"view\": monitoring_v3.ListTimeSeriesRequest.TimeSeriesView.FULL,\n",
    "    }\n",
    ")\n",
    "results_io_read = client.list_time_series(\n",
    "    request={\n",
    "        \"name\": project_name,\n",
    "        \"filter\": 'metric.type = \"compute.googleapis.com/instance/disk/read_bytes_count\"',\n",
    "        \"interval\": interval,\n",
    "        \"view\": monitoring_v3.ListTimeSeriesRequest.TimeSeriesView.FULL,\n",
    "    }\n",
    ")\n",
    "results_io_write = client.list_time_series(\n",
    "    request={\n",
    "        \"name\": project_name,\n",
    "        \"filter\": 'metric.type = \"compute.googleapis.com/instance/disk/write_bytes_count\"',\n",
    "        \"interval\": interval,\n",
    "        \"view\": monitoring_v3.ListTimeSeriesRequest.TimeSeriesView.FULL,\n",
    "    }\n",
    ")\n",
    "results_iops_read = client.list_time_series(\n",
    "    request={\n",
    "        \"name\": project_name,\n",
    "        \"filter\": 'metric.type = \"compute.googleapis.com/instance/disk/read_ops_count\"',\n",
    "        \"interval\": interval,\n",
    "        \"view\": monitoring_v3.ListTimeSeriesRequest.TimeSeriesView.FULL,\n",
    "    }\n",
    ")\n",
    "results_iops_write = client.list_time_series(\n",
    "    request={\n",
    "        \"name\": project_name,\n",
    "        \"filter\": 'metric.type = \"compute.googleapis.com/instance/disk/write_ops_count\"',\n",
    "        \"interval\": interval,\n",
    "        \"view\": monitoring_v3.ListTimeSeriesRequest.TimeSeriesView.FULL,\n",
    "    }\n",
    ")\n",
    "\n",
    "def loadMonitoringData(experiment): \n",
    "    df = pd.DataFrame()\n",
    "    set_timestamp_column = True\n",
    "    first_len = 0\n",
    "    for ts_cpu, ts_io_read, ts_io_write, ts_iops_read, ts_iops_write in zip(results_cpu, results_io_read, results_io_write, results_iops_read, results_iops_write):\n",
    "        # print(ts)\n",
    "        label = ts_cpu.metric.labels[\"instance_name\"]\n",
    "        # print(ts_cpu)\n",
    "        if \"orchestrator\" in label:\n",
    "            # Skip Orchesrtator\n",
    "            continue\n",
    "        if \"experiment-syncmesh-with-latency-6\" in label:\n",
    "            # Skip Orchesrtator\n",
    "            continue\n",
    "        if label.startswith(experiment):\n",
    "            print(label + \": \" + str(len(ts_cpu.points)))\n",
    "            if set_timestamp_column:\n",
    "                first_len=len(ts_cpu.points)-1\n",
    "                print(first_len)\n",
    "                df['timestamp'] = pd.to_datetime([p.interval.start_time.ToDatetime() for p in ts_cpu.points[:first_len]])\n",
    "                set_timestamp_column = False\n",
    "            # print(ts.points[0])\n",
    "            # When deploying the vm they might take different amount of time leading to some values beeing available a minute early this leading to different length\n",
    "            # We can trim off the last values as they are orderer from most recent to last\n",
    "            df['cpu_util_' + label] = [p.value.double_value for p in ts_cpu.points[:first_len]]\n",
    "            df['io_read_' + label] = [p.value.int64_value for p in ts_io_read.points[:first_len]]\n",
    "            df['io_write_' + label] = [p.value.int64_value for p in ts_io_write.points[:first_len]]\n",
    "            df['iops_read_' + label] = [p.value.int64_value for p in ts_iops_read.points[:first_len]]\n",
    "            df['iops_write_' + label] = [p.value.int64_value for p in ts_iops_write.points[:first_len]]\n",
    "\n",
    "\n",
    "    df.set_index('timestamp', inplace=True)\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    columns = df.columns\n",
    "    df.to_csv(f\"{outdir}/monitoring_{experiment}.csv\")\n",
    "    print(f\"Gathered all Monitoring data for {experiment}\")\n",
    "    return df\n",
    "\n",
    "for experiment in sharedVariables.experiments: \n",
    "    try: \n",
    "        loadMonitoringData(experiment)\n",
    "    except Exception as e: \n",
    "        print(e)\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "experiment-baseline-with-latency-node-instance-2: 57\n",
      "56\n",
      "experiment-baseline-with-latency-node-instance-3: 57\n",
      "experiment-baseline-with-latency-node-instance-1: 57\n",
      "experiment-baseline-with-latency-central-server-instance: 57\n",
      "experiment-baseline-with-latency-client-instance: 57\n",
      "Gathered all Monitoring data for experiment-baseline-with-latency\n",
      "experiment-syncmesh-with-latency-node-instance-2: 49\n",
      "48\n",
      "experiment-syncmesh-with-latency-node-instance-3: 48\n",
      "experiment-syncmesh-with-latency-node-instance-1: 48\n",
      "experiment-syncmesh-with-latency-client-instance: 48\n",
      "Gathered all Monitoring data for experiment-syncmesh-with-latency\n",
      "\"None of ['timestamp'] are in the columns\"\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3352614b04f6a771ec3a392566a3763a34cdc96a177f762027aa589f02ec5f67"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 32-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}