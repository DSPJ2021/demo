{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# Following variables can be set here or via papermill\n",
    "# experiment = \"experiment-baseline-with-latency-3\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Imports"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "from google.oauth2 import service_account\n",
    "from google.cloud import monitoring_v3\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud.bigquery.job import ExtractJobConfig\n",
    "import time\n",
    "import matplotlib.dates as mdates\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import os\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import shutil\n",
    "import subprocess\n",
    "from subprocess import PIPE\n",
    "import sharedVariables\n",
    "from sharedVariables import *"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Network Logs"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    '../infrastructure/credentials.json')\n",
    "\n",
    "client = bigquery.Client(credentials=credentials)\n",
    "today = datetime.datetime.today()\n",
    "table_id = f\"compute_googleapis_com_vpc_flows_{today.strftime('%Y%m%d')}\"\n",
    "# table_id = 'compute_googleapis_com_vpc_flows_20210720'\n",
    "blob_name = \"export.log\"\n",
    "\n",
    "\n",
    "\n",
    "extract_conf = ExtractJobConfig()\n",
    "extract_conf.compression = 'NONE'\n",
    "extract_conf.destination_format = 'NEWLINE_DELIMITED_JSON'\n",
    "\n",
    "def getDataset(experiment, outDir): \n",
    "    ensureDirectory(outDir)\n",
    "    bucket_name = experiment + \"-log-bucket\"\n",
    "\n",
    "    destination_uri = \"gs://{}/{}\".format(bucket_name, blob_name)\n",
    "    dataset_ref = bigquery.DatasetReference(project, experiment.replace(\"-\", \"_\"))\n",
    "    table_ref = dataset_ref.table(table_id)\n",
    "\n",
    "    extract_job = client.extract_table(\n",
    "        table_ref,\n",
    "        destination_uri,\n",
    "        # Location must match that of the source table.\n",
    "        location=\"US\",\n",
    "        job_config=extract_conf\n",
    "    )  # API request\n",
    "    extract_job.result()  # Waits for job to complete.\n",
    "\n",
    "    print(\n",
    "        \"Exported {}:{}.{} to {}\".format(project, experiment, table_id, destination_uri)\n",
    "    )\n",
    "\n",
    "\n",
    "    storage_client = storage.Client(credentials=credentials)\n",
    "\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "    blob.download_to_filename(f\"{outdir}/gcp-flow-network.log\")\n",
    "\n",
    "    print(\n",
    "        \"Blob downloaded successfully.\"\n",
    "    )\n",
    "\n",
    "if 'experiment' not in locals():\n",
    "    for experiment in sharedVariables.experiments: \n",
    "        try: \n",
    "            getDataset(experiment)\n",
    "        except Exception as e: \n",
    "            print(e)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# PCAPs"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "def download_pcap(ip, outDir):\n",
    "    ensureDirectory(outDir)\n",
    "    print(\"Dowloading File\")\n",
    "    print(os.path.join(Path.cwd(), \"..\", \"cert\"))\n",
    "    result = subprocess.run(f\"scp -i ../infrastructure/orchestrator.pem -o StrictHostKeyChecking=no dnhb@{ip}:/captures.zip ./captures.zip\", shell=True, stdout=PIPE, stderr=PIPE, universal_newlines=True)\n",
    "    if(result.returncode == 0):\n",
    "        print(result.stdout)\n",
    "    else:\n",
    "        raise Exception(result.stderr)\n",
    "    shutil.unpack_archive(\"./captures.zip\", outdir)\n",
    "\n",
    "if 'experiment' not in locals():\n",
    "    download_pcap(\"35.224.133.98\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Monitoring"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    '../infrastructure/credentials.json')\n",
    "\n",
    "# Our project ID\n",
    "\n",
    "monitoring_client = monitoring_v3.MetricServiceClient(credentials=credentials)\n",
    "project_name = f\"projects/{project_id}\"\n",
    "now = time.time()\n",
    "seconds = int(now)\n",
    "nanos = int((now - seconds) * 10 ** 9)\n",
    "interval = monitoring_v3.TimeInterval(\n",
    "    {\n",
    "        \"end_time\": {\"seconds\": seconds, \"nanos\": nanos},\n",
    "        # 3600 = Get the last hour of metrics\n",
    "        \"start_time\": {\"seconds\": (seconds - (3600 * 9)), \"nanos\": nanos},\n",
    "    }\n",
    ")\n",
    "\n",
    "# Add Filter? metric.label.instance_name = \"your-instance-id\"\n",
    "results_cpu = monitoring_client.list_time_series(\n",
    "    request={\n",
    "        \"name\": project_name,\n",
    "        \"filter\": 'metric.type =  \"compute.googleapis.com/instance/cpu/utilization\"',\n",
    "        \"interval\": interval,\n",
    "        \"view\": monitoring_v3.ListTimeSeriesRequest.TimeSeriesView.FULL,\n",
    "    }\n",
    ")\n",
    "results_io_read = monitoring_client.list_time_series(\n",
    "    request={\n",
    "        \"name\": project_name,\n",
    "        \"filter\": 'metric.type = \"compute.googleapis.com/instance/disk/read_bytes_count\"',\n",
    "        \"interval\": interval,\n",
    "        \"view\": monitoring_v3.ListTimeSeriesRequest.TimeSeriesView.FULL,\n",
    "    }\n",
    ")\n",
    "results_io_write = monitoring_client.list_time_series(\n",
    "    request={\n",
    "        \"name\": project_name,\n",
    "        \"filter\": 'metric.type = \"compute.googleapis.com/instance/disk/write_bytes_count\"',\n",
    "        \"interval\": interval,\n",
    "        \"view\": monitoring_v3.ListTimeSeriesRequest.TimeSeriesView.FULL,\n",
    "    }\n",
    ")\n",
    "results_iops_read = monitoring_client.list_time_series(\n",
    "    request={\n",
    "        \"name\": project_name,\n",
    "        \"filter\": 'metric.type = \"compute.googleapis.com/instance/disk/read_ops_count\"',\n",
    "        \"interval\": interval,\n",
    "        \"view\": monitoring_v3.ListTimeSeriesRequest.TimeSeriesView.FULL,\n",
    "    }\n",
    ")\n",
    "results_iops_write = monitoring_client.list_time_series(\n",
    "    request={\n",
    "        \"name\": project_name,\n",
    "        \"filter\": 'metric.type = \"compute.googleapis.com/instance/disk/write_ops_count\"',\n",
    "        \"interval\": interval,\n",
    "        \"view\": monitoring_v3.ListTimeSeriesRequest.TimeSeriesView.FULL,\n",
    "    }\n",
    ")\n",
    "\n",
    "def loadMonitoringData(experiment, outDir): \n",
    "    ensureDirectory(outDir)\n",
    "    df = pd.DataFrame()\n",
    "    set_timestamp_column = True\n",
    "    first_len = 0\n",
    "    for ts_cpu, ts_io_read, ts_io_write, ts_iops_read, ts_iops_write in zip(results_cpu, results_io_read, results_io_write, results_iops_read, results_iops_write):\n",
    "        # print(ts)\n",
    "        label = ts_cpu.metric.labels[\"instance_name\"]\n",
    "        # print(ts_cpu)\n",
    "        if \"orchestrator\" in label:\n",
    "            # Skip Orchesrtator\n",
    "            continue\n",
    "        if label.startswith(experiment):\n",
    "            print(label + \": \" + str(len(ts_cpu.points)))\n",
    "            if set_timestamp_column:\n",
    "                first_len=len(ts_cpu.points)-1\n",
    "                print(first_len)\n",
    "                df['timestamp'] = pd.to_datetime([p.interval.start_time.ToDatetime() for p in ts_cpu.points[:first_len]])\n",
    "                set_timestamp_column = False\n",
    "            # print(ts.points[0])\n",
    "            # When deploying the vm they might take different amount of time leading to some values beeing available a minute early this leading to different length\n",
    "            # We can trim off the last values as they are orderer from most recent to last\n",
    "            df['cpu_util_' + label] = [p.value.double_value for p in ts_cpu.points[:first_len]]\n",
    "            df['io_read_' + label] = [p.value.int64_value for p in ts_io_read.points[:first_len]]\n",
    "            df['io_write_' + label] = [p.value.int64_value for p in ts_io_write.points[:first_len]]\n",
    "            df['iops_read_' + label] = [p.value.int64_value for p in ts_iops_read.points[:first_len]]\n",
    "            df['iops_write_' + label] = [p.value.int64_value for p in ts_iops_write.points[:first_len]]\n",
    "\n",
    "    df.set_index('timestamp', inplace=True)\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    columns = df.columns\n",
    "    df.to_csv(f\"{outdir}/monitoring.csv\")\n",
    "    print(f\"Gathered all Monitoring data for {experiment}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# Make sure your experiment name is included in the experiments list, otherwise only errors will occur.\n",
    "if 'experiment' not in locals():\n",
    "    for experiment in sharedVariables.experiments: \n",
    "        try: \n",
    "            loadMonitoringData(experiment)\n",
    "        except Exception as e: \n",
    "            print(e)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Get TimeStamps"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "# # Workaround for now\n",
    "# # df_sync3 = pd.read_csv(f'{outdir}/experiment-syncmesh-with-latency-3.csv')\n",
    "# # df_base3 = pd.read_csv(f'{outdir}/experiment-baseline-with-latency-3.csv')\n",
    "# df_sync3 = loadData(f'{outdir}/experiment-syncmesh-with-latency-3.log')\n",
    "# df_base3 = loadData(f'{outdir}/experiment-baseline-with-latency-3.log')\n",
    "# # df_base3.set_index('timestamp', inplace=True)\n",
    "# # df_sync3.set_index('timestamp', inplace=True)\n",
    "\n",
    "# # df_sync6 = loadData(f'{outdir}/experiment-syncmesh-with-latency-6.log')\n",
    "# # df_base6 = loadData(f'{outdir}/experiment-baseline-with-latency-6.log')\n",
    "\n",
    "# def filterDataForSeperator(df): \n",
    "#     df = df[df[\"jsonPayload.connection.src_ip\"].isin([ip_seperator, ip_orchestrator])]\n",
    "#     df = df[df[\"jsonPayload.connection.dest_ip\"].isin([ip_seperator, ip_orchestrator])]\n",
    "#     df = df[df[\"jsonPayload.connection.dest_port\"] == 443]\n",
    "#     return df\n",
    "\n",
    "# seperator_base3 = filterDataForSeperator(df_base3)\n",
    "# seperator_sync3 = filterDataForSeperator(df_sync3)\n",
    "\n",
    "# # df = df[df[\"jsonPayload.connection.dest_ip\"]]\n",
    "# print(seperator_sync3.index)\n",
    "# print(seperator_base3.index)\n",
    "# # df_base3.info()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "# Script for automatic data retrieval via papermill\n",
    "if 'experiment' in locals():\n",
    "    todaystring = datetime.datetime.today().strftime('%Y%m%d-%H')\n",
    "    outdir = f\"data/{todaystring}-{experiment}\"\n",
    "    download_pcap(\"35.224.133.98\", outDir=outdir)\n",
    "    loadMonitoringData(experiment, outDir=outdir)\n",
    "    getDataset(experiment, outDir=outdir)\n",
    "\n",
    "    "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "experiment-baseline-with-latency-3-node-instance-2: 31\n",
      "30\n",
      "experiment-baseline-with-latency-3-node-instance-2: 93\n",
      "experiment-baseline-with-latency-3-node-instance-3: 93\n",
      "experiment-baseline-with-latency-3-node-instance-3: 31\n",
      "experiment-baseline-with-latency-3-node-instance-1: 30\n",
      "experiment-baseline-with-latency-3-node-instance-1: 92\n",
      "experiment-baseline-with-latency-3-client-instance: 91\n",
      "experiment-baseline-with-latency-3-client-instance: 30\n",
      "experiment-baseline-with-latency-3-central-server: 30\n",
      "experiment-baseline-with-latency-3-central-server: 89\n",
      "Gathered all Monitoring data for experiment-baseline-with-latency-3\n",
      "Exported dspj-315716:experiment-baseline-with-latency-3.compute_googleapis_com_vpc_flows_20210919 to gs://experiment-baseline-with-latency-3-log-bucket/export.log\n",
      "Blob downloaded successfully.\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9518128f597d7b00dc14729602cfd87fb7b2cf75925976bcb0d0e328a830a12b"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}