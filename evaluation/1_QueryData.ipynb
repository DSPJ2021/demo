{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following variables can be set here or via papermill\n",
    "if ci != \"True\":\n",
    "    print(\"Running in non-CI mode\")\n",
    "    # experiment = \"experiment-distributed-gundb-with-latency-3\"\n",
    "\n",
    "print(ci)\n",
    "print(experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.oauth2 import service_account\n",
    "from google.cloud import monitoring_v3\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud.bigquery.job import ExtractJobConfig\n",
    "import time\n",
    "import matplotlib.dates as mdates\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import os\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import shutil\n",
    "import subprocess\n",
    "from subprocess import PIPE\n",
    "import sharedVariables\n",
    "from sharedVariables import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    '../infrastructure/credentials.json')\n",
    "\n",
    "client = bigquery.Client(credentials=credentials)\n",
    "today = datetime.datetime.today()\n",
    "table_id = f\"compute_googleapis_com_vpc_flows_{today.strftime('%Y%m%d')}\"\n",
    "# table_id = 'compute_googleapis_com_vpc_flows_20210720'\n",
    "blob_name = \"export.log\"\n",
    "\n",
    "\n",
    "\n",
    "extract_conf = ExtractJobConfig()\n",
    "extract_conf.compression = 'NONE'\n",
    "extract_conf.destination_format = 'NEWLINE_DELIMITED_JSON'\n",
    "\n",
    "def getDataset(experiment, outDir = datetime.datetime.today().strftime('%Y%m%d-%H')): \n",
    "    ensureDirectory(outDir)\n",
    "    bucket_name = experiment + \"-log-bucket\"\n",
    "\n",
    "    destination_uri = \"gs://{}/{}\".format(bucket_name, blob_name)\n",
    "    dataset_ref = bigquery.DatasetReference(project, experiment.replace(\"-\", \"_\"))\n",
    "    table_ref = dataset_ref.table(table_id)\n",
    "\n",
    "    extract_job = client.extract_table(\n",
    "        table_ref,\n",
    "        destination_uri,\n",
    "        # Location must match that of the source table.\n",
    "        location=\"US\",\n",
    "        job_config=extract_conf\n",
    "    )  # API request\n",
    "    extract_job.result()  # Waits for job to complete.\n",
    "\n",
    "    print(\n",
    "        \"Exported {}:{}.{} to {}\".format(project, experiment, table_id, destination_uri)\n",
    "    )\n",
    "\n",
    "\n",
    "    storage_client = storage.Client(credentials=credentials)\n",
    "\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "    blob.download_to_filename(f\"{outDir}/gcp-flow-network.log\")\n",
    "\n",
    "    print(\n",
    "        \"Blob downloaded successfully.\"\n",
    "    )\n",
    "\n",
    "if 'experiment' not in locals():\n",
    "    for experiment in sharedVariables.experiments: \n",
    "        try: \n",
    "            getDataset(experiment)\n",
    "        except Exception as e: \n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCAPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_pcap(ip, outDir = datetime.datetime.today().strftime('%Y%m%d-%H')):\n",
    "    ensureDirectory(outDir)\n",
    "    print(\"Dowloading File\")\n",
    "    print(os.path.join(Path.cwd(), \"..\", \"cert\"))\n",
    "    result = subprocess.run(f\"scp -i ../infrastructure/orchestrator.pem -o StrictHostKeyChecking=no orchestrator@{ip}:/captures.zip ./captures.zip\", shell=True, stdout=PIPE, stderr=PIPE, universal_newlines=True)\n",
    "    if(result.returncode == 0):\n",
    "        print(result.stdout)\n",
    "    else:\n",
    "        raise Exception(result.stderr)\n",
    "    shutil.unpack_archive(\"./captures.zip\", outDir)\n",
    "\n",
    "if 'experiment' not in locals():\n",
    "    download_pcap(\"35.224.133.98\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timeframe: 21600\n",
      "['cpu_util_', 'io_read_', 'io_write_', 'iops_read_', 'iops_write_', 'mem_buffered_', 'mem_cached_', 'mem_free_', 'mem_slab_', 'mem_used_', 'mem_perc_buffered_', 'mem_perc_cached_', 'mem_perc_free_', 'mem_perc_slab_', 'mem_perc_used_']\n"
     ]
    }
   ],
   "source": [
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    '../infrastructure/credentials.json')\n",
    "\n",
    "# Our project ID\n",
    "\n",
    "monitoring_client = monitoring_v3.MetricServiceClient(credentials=credentials)\n",
    "project_name = f\"projects/{project_id}\"\n",
    "now = time.time()\n",
    "seconds = int(now)\n",
    "nanos = int((now - seconds) * 10 ** 9)\n",
    "\n",
    "# 3600 = Get the last hour of metrics\n",
    "timeframe = 3600 * 3\n",
    "if 'experiment' not in locals():\n",
    "    print(\"set timeframe to one hour\")\n",
    "    timeframe = 3600 * 1\n",
    "\n",
    "print(\"Timeframe:\", timeframe)\n",
    "interval = monitoring_v3.TimeInterval(\n",
    "    {\n",
    "        \"end_time\": {\"seconds\": seconds, \"nanos\": nanos},\n",
    "        \"start_time\": {\"seconds\": (seconds - timeframe), \"nanos\": nanos},\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "# Add Filter? metric.label.instance_name = \"your-instance-id\"\n",
    "results_cpu = monitoring_client.list_time_series(\n",
    "    request={\n",
    "        \"name\": project_name,\n",
    "        \"filter\": 'metric.type =  \"compute.googleapis.com/instance/cpu/utilization\"',\n",
    "        \"interval\": interval,\n",
    "        \"view\": monitoring_v3.ListTimeSeriesRequest.TimeSeriesView.FULL,\n",
    "    }\n",
    ")\n",
    "results_io_read = monitoring_client.list_time_series(\n",
    "    request={\n",
    "        \"name\": project_name,\n",
    "        \"filter\": 'metric.type = \"compute.googleapis.com/instance/disk/read_bytes_count\"',\n",
    "        \"interval\": interval,\n",
    "        \"view\": monitoring_v3.ListTimeSeriesRequest.TimeSeriesView.FULL,\n",
    "    }\n",
    ")\n",
    "results_io_write = monitoring_client.list_time_series(\n",
    "    request={\n",
    "        \"name\": project_name,\n",
    "        \"filter\": 'metric.type = \"compute.googleapis.com/instance/disk/write_bytes_count\"',\n",
    "        \"interval\": interval,\n",
    "        \"view\": monitoring_v3.ListTimeSeriesRequest.TimeSeriesView.FULL,\n",
    "    }\n",
    ")\n",
    "results_iops_read = monitoring_client.list_time_series(\n",
    "    request={\n",
    "        \"name\": project_name,\n",
    "        \"filter\": 'metric.type = \"compute.googleapis.com/instance/disk/read_ops_count\"',\n",
    "        \"interval\": interval,\n",
    "        \"view\": monitoring_v3.ListTimeSeriesRequest.TimeSeriesView.FULL,\n",
    "    }\n",
    ")\n",
    "results_iops_write = monitoring_client.list_time_series(\n",
    "    request={\n",
    "        \"name\": project_name,\n",
    "        \"filter\": 'metric.type = \"compute.googleapis.com/instance/disk/write_ops_count\"',\n",
    "        \"interval\": interval,\n",
    "        \"view\": monitoring_v3.ListTimeSeriesRequest.TimeSeriesView.FULL,\n",
    "    }\n",
    ")\n",
    "\n",
    "mutli_results_memory = {}\n",
    "mem_states = [\"buffered\", \"cached\", \"free\", \"slab\", \"used\"]\n",
    "for state in mem_states:\n",
    "    mutli_results_memory[state] = monitoring_client.list_time_series(\n",
    "        request={\n",
    "            \"name\": project_name,\n",
    "            \"filter\": f'metric.type = \"agent.googleapis.com/memory/bytes_used\" AND metric.labels.state = \"{state}\"',\n",
    "            \"interval\": interval,\n",
    "            \"view\": monitoring_v3.ListTimeSeriesRequest.TimeSeriesView.FULL,\n",
    "        }\n",
    "    )\n",
    "\n",
    "mutli_results_memory_percentage = {}\n",
    "for state in mem_states:\n",
    "    mutli_results_memory_percentage[state] = monitoring_client.list_time_series(\n",
    "        request={\n",
    "            \"name\": project_name,\n",
    "            \"filter\": f'metric.type = \"agent.googleapis.com/memory/percent_used\" AND metric.labels.state = \"{state}\"',\n",
    "            \"interval\": interval,\n",
    "            \"view\": monitoring_v3.ListTimeSeriesRequest.TimeSeriesView.FULL,\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "# Prepare Data so that as the monitoring time_series_list is not aligned\n",
    "time_series_list_list_labels = [\"cpu_util_\", \"io_read_\", \"io_write_\", \"iops_read_\", \"iops_write_\"]\n",
    "time_series_list_list = [results_cpu, results_io_read, results_io_write, results_iops_read, results_iops_write]\n",
    "# Add Memory\n",
    "for state in mem_states:\n",
    "    time_series_list_list.append(mutli_results_memory[state])\n",
    "    time_series_list_list_labels.append(f\"mem_{state}_\")\n",
    "\n",
    "for state in mem_states:\n",
    "    time_series_list_list.append(mutli_results_memory_percentage[state])\n",
    "    time_series_list_list_labels.append(f\"mem_perc_{state}_\")\n",
    "\n",
    "print(time_series_list_list_labels)\n",
    "\n",
    "instance_logs = {}\n",
    "\n",
    "for index, time_series_list in enumerate(time_series_list_list):\n",
    "    for time_series in time_series_list:\n",
    "        if time_series.metric.labels[\"instance_name\"]:\n",
    "            label = time_series.metric.labels[\"instance_name\"]\n",
    "        else:\n",
    "            # Find out label by comapring the instance ID\n",
    "            for compare_item in time_series_list_list[0]:\n",
    "                if compare_item.resource.labels[\"instance_id\"] == time_series.resource.labels[\"instance_id\"]:\n",
    "                    label = compare_item.metric.labels[\"instance_name\"]\n",
    "        if label not in instance_logs:\n",
    "            instance_logs[label] = np.empty(len(time_series_list_list), dtype=object)\n",
    "        instance_logs[label][index] = time_series\n",
    "\n",
    "def loadMonitoringData(experiment, outDir = datetime.datetime.today().strftime('%Y%m%d-%H')): \n",
    "    ensureDirectory(outDir)\n",
    "    df = pd.DataFrame()\n",
    "    set_timestamp_column = True\n",
    "    first_len = 0\n",
    "    # Filter instances for our experiment\n",
    "    filteredInstances = {}\n",
    "    for key, instance in instance_logs.items():\n",
    "        if key.startswith(experiment):\n",
    "            filteredInstances[key] = instance\n",
    "\n",
    "    # Find out which point array is smallest\n",
    "    first_len = min([len(item.points) for instances in filteredInstances.values() for item in instances])\n",
    "    for instances in filteredInstances.values():\n",
    "        print(instances[0].metric.labels[\"instance_name\"], len(instances[0].points))\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Overall min len:\", first_len)\n",
    "\n",
    "    for key, instances in filteredInstances.items():\n",
    "        for index in range(len(instances)):\n",
    "            label = key\n",
    "            # if \"orchestrator\" in label:\n",
    "            #     # Skip Orchesrtator\n",
    "            #     continue\n",
    "            if set_timestamp_column:\n",
    "                # first_len = min([len(item.points) - 1 for item in instances])\n",
    "                # print(first_len)\n",
    "                print(\"SET min len:\", first_len)\n",
    "\n",
    "                df['timestamp'] = pd.to_datetime([p.interval.start_time.ToDatetime() for p in instances[0].points[:first_len]])\n",
    "                set_timestamp_column = False\n",
    "            # print(ts.points[0])\n",
    "            # When deploying the vm they might take different amount of time leading to some values beeing available a minute early this leading to different length\n",
    "            # We can trim off the last values as they are orderer from most recent to last\n",
    "            df[time_series_list_list_labels[index] + label] = [p.value.double_value for p in instances[index].points[:first_len]]\n",
    "\n",
    "            # df['cpu_util_' + label] = [p.value.double_value for p in ts_cpu.points[:first_len]]\n",
    "            # df['io_read_' + label] = [p.value.int64_value for p in ts_io_read.points[:first_len]]\n",
    "            # df['io_write_' + label] = [p.value.int64_value for p in ts_io_write.points[:first_len]]\n",
    "            # df['iops_read_' + label] = [p.value.int64_value for p in ts_iops_read.points[:first_len]]\n",
    "            # df['iops_write_' + label] = [p.value.int64_value for p in ts_iops_write.points[:first_len]]\n",
    "\n",
    "    df.set_index('timestamp', inplace=True)\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    columns = df.columns\n",
    "    df.to_csv(f\"{outDir}/monitoring.csv\")\n",
    "    print(f\"Gathered all Monitoring data for {experiment}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# Make sure your experiment name is included in the experiments list, otherwise only errors will occur.\n",
    "if 'experiment' not in locals():\n",
    "    for experiment in sharedVariables.experiments: \n",
    "        try: \n",
    "            loadMonitoringData(experiment)\n",
    "        except Exception as e: \n",
    "            print(e)\n",
    "# loadMonitoringData(\"experiment-syncmesh-with-latency-9\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get TimeStamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Workaround for now\n",
    "# # df_sync3 = pd.read_csv(f'{outdir}/experiment-syncmesh-with-latency-3.csv')\n",
    "# # df_base3 = pd.read_csv(f'{outdir}/experiment-baseline-with-latency-3.csv')\n",
    "# df_sync3 = loadData(f'{outdir}/experiment-syncmesh-with-latency-3.log')\n",
    "# df_base3 = loadData(f'{outdir}/experiment-baseline-with-latency-3.log')\n",
    "# # df_base3.set_index('timestamp', inplace=True)\n",
    "# # df_sync3.set_index('timestamp', inplace=True)\n",
    "\n",
    "# # df_sync6 = loadData(f'{outdir}/experiment-syncmesh-with-latency-6.log')\n",
    "# # df_base6 = loadData(f'{outdir}/experiment-baseline-with-latency-6.log')\n",
    "\n",
    "# def filterDataForSeperator(df): \n",
    "#     df = df[df[\"jsonPayload.connection.src_ip\"].isin([ip_seperator, ip_orchestrator])]\n",
    "#     df = df[df[\"jsonPayload.connection.dest_ip\"].isin([ip_seperator, ip_orchestrator])]\n",
    "#     df = df[df[\"jsonPayload.connection.dest_port\"] == 443]\n",
    "#     return df\n",
    "\n",
    "# seperator_base3 = filterDataForSeperator(df_base3)\n",
    "# seperator_sync3 = filterDataForSeperator(df_sync3)\n",
    "\n",
    "# # df = df[df[\"jsonPayload.connection.dest_ip\"]]\n",
    "# print(seperator_sync3.index)\n",
    "# print(seperator_base3.index)\n",
    "# # df_base3.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment-distributed-gundb-with-latency-3-node-instance-2 188\n",
      "experiment-distributed-gundb-with-latency-3-node-instance-1 188\n",
      "experiment-distributed-gundb-with-latency-3-node-instance-3 187\n",
      "experiment-distributed-gundb-with-latency-3-client-instance 188\n",
      "experiment-distributed-gundb-with-latency-3-test-orchestrator 185\n",
      "Overall min len: 185\n",
      "SET min len: 185\n",
      "Gathered all Monitoring data for experiment-distributed-gundb-with-latency-3\n"
     ]
    },
    {
     "ename": "NotFound",
     "evalue": "404 POST https://bigquery.googleapis.com/bigquery/v2/projects/dspj-315716/jobs?prettyPrint=false: Not found: Dataset dspj-315716:experiment_distributed_gundb_with_latency_3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFound\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3248/3759914534.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m# download_pcap(ip, outDir=outdir)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mloadMonitoringData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexperiment\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutDir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutdir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mgetDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexperiment\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutDir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutdir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3248/3592424379.py\u001b[0m in \u001b[0;36mgetDataset\u001b[1;34m(experiment, outDir)\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mtable_ref\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset_ref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtable_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     extract_job = client.extract_table(\n\u001b[0m\u001b[0;32m     25\u001b[0m         \u001b[0mtable_ref\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mdestination_uri\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\google\\cloud\\bigquery\\client.py\u001b[0m in \u001b[0;36mextract_table\u001b[1;34m(self, source, destination_uris, job_id, job_id_prefix, location, project, job_config, retry, timeout, source_type)\u001b[0m\n\u001b[0;32m   3135\u001b[0m             \u001b[0mjob_ref\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdestination_uris\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclient\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjob_config\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mjob_config\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3136\u001b[0m         )\n\u001b[1;32m-> 3137\u001b[1;33m         \u001b[0mextract_job\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mretry\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3139\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mextract_job\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\google\\cloud\\bigquery\\job\\base.py\u001b[0m in \u001b[0;36m_begin\u001b[1;34m(self, client, retry, timeout)\u001b[0m\n\u001b[0;32m    459\u001b[0m         \u001b[1;31m# job has an ID.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    460\u001b[0m         \u001b[0mspan_attributes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"path\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 461\u001b[1;33m         api_response = client._call_api(\n\u001b[0m\u001b[0;32m    462\u001b[0m             \u001b[0mretry\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    463\u001b[0m             \u001b[0mspan_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"BigQuery.job.begin\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\google\\cloud\\bigquery\\client.py\u001b[0m in \u001b[0;36m_call_api\u001b[1;34m(self, retry, span_name, span_attributes, job_ref, **kwargs)\u001b[0m\n\u001b[0;32m    739\u001b[0m                 \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mspan_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattributes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mspan_attributes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclient\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjob_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mjob_ref\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    740\u001b[0m             ):\n\u001b[1;32m--> 741\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    742\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    743\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\google\\api_core\\retry.py\u001b[0m in \u001b[0;36mretry_wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    284\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maximum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmultiplier\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_multiplier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m             )\n\u001b[1;32m--> 286\u001b[1;33m             return retry_target(\n\u001b[0m\u001b[0;32m    287\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_predicate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\google\\api_core\\retry.py\u001b[0m in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, deadline, on_error)\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0msleep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msleep_generator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 189\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m         \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\google\\cloud\\_http.py\u001b[0m in \u001b[0;36mapi_request\u001b[1;34m(self, method, path, query_params, data, content_type, headers, api_base_url, api_version, expect_json, _target_object, timeout)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    483\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;36m200\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 484\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_http_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    485\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    486\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mexpect_json\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFound\u001b[0m: 404 POST https://bigquery.googleapis.com/bigquery/v2/projects/dspj-315716/jobs?prettyPrint=false: Not found: Dataset dspj-315716:experiment_distributed_gundb_with_latency_3"
     ]
    }
   ],
   "source": [
    "# Script for automatic data retrieval via papermill\n",
    "if 'experiment' in locals():\n",
    "    todaystring = datetime.datetime.today().strftime('%Y%m%d-%H')\n",
    "    outdir = f\"data/{todaystring}-{experiment}\"\n",
    "\n",
    "    f = open(os.path.join(Path.cwd(), \"..\", \"infrastructure\", \"orchestrator.txt\"), \"r\")\n",
    "    ip = f.read()\n",
    "    download_pcap(ip, outDir=outdir)\n",
    "    loadMonitoringData(experiment, outDir=outdir)\n",
    "    getDataset(experiment, outDir=outdir)\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
