{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# Following variables can be set here or via papermill\n",
    "# experiment = \"experiment-baseline-with-latency-3\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Imports"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from google.oauth2 import service_account\n",
    "from google.cloud import monitoring_v3\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud.bigquery.job import ExtractJobConfig\n",
    "import time\n",
    "import matplotlib.dates as mdates\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import os\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import shutil\n",
    "import subprocess\n",
    "from subprocess import PIPE\n",
    "import sharedVariables\n",
    "from sharedVariables import *"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Network Logs"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    '../infrastructure/credentials.json')\n",
    "\n",
    "client = bigquery.Client(credentials=credentials)\n",
    "today = datetime.datetime.today()\n",
    "table_id = f\"compute_googleapis_com_vpc_flows_{today.strftime('%Y%m%d')}\"\n",
    "# table_id = 'compute_googleapis_com_vpc_flows_20210720'\n",
    "blob_name = \"export.log\"\n",
    "\n",
    "\n",
    "\n",
    "extract_conf = ExtractJobConfig()\n",
    "extract_conf.compression = 'NONE'\n",
    "extract_conf.destination_format = 'NEWLINE_DELIMITED_JSON'\n",
    "\n",
    "def getDataset(experiment, outDir = datetime.datetime.today().strftime('%Y%m%d-%H')): \n",
    "    ensureDirectory(outDir)\n",
    "    bucket_name = experiment + \"-log-bucket\"\n",
    "\n",
    "    destination_uri = \"gs://{}/{}\".format(bucket_name, blob_name)\n",
    "    dataset_ref = bigquery.DatasetReference(project, experiment.replace(\"-\", \"_\"))\n",
    "    table_ref = dataset_ref.table(table_id)\n",
    "\n",
    "    extract_job = client.extract_table(\n",
    "        table_ref,\n",
    "        destination_uri,\n",
    "        # Location must match that of the source table.\n",
    "        location=\"US\",\n",
    "        job_config=extract_conf\n",
    "    )  # API request\n",
    "    extract_job.result()  # Waits for job to complete.\n",
    "\n",
    "    print(\n",
    "        \"Exported {}:{}.{} to {}\".format(project, experiment, table_id, destination_uri)\n",
    "    )\n",
    "\n",
    "\n",
    "    storage_client = storage.Client(credentials=credentials)\n",
    "\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "    blob.download_to_filename(f\"{outDir}/gcp-flow-network.log\")\n",
    "\n",
    "    print(\n",
    "        \"Blob downloaded successfully.\"\n",
    "    )\n",
    "\n",
    "if 'experiment' not in locals():\n",
    "    for experiment in sharedVariables.experiments: \n",
    "        try: \n",
    "            getDataset(experiment)\n",
    "        except Exception as e: \n",
    "            print(e)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Exported dspj-315716:experiment-baseline-with-latency-3.compute_googleapis_com_vpc_flows_20210920 to gs://experiment-baseline-with-latency-3-log-bucket/export.log\n",
      "Blob downloaded successfully.\n",
      "404 Not found: Table dspj-315716:experiment_syncmesh_with_latency_3.compute_googleapis_com_vpc_flows_20210920 was not found in location US\n",
      "404 Not found: Table dspj-315716:experiment_advanced_mongo_with_latency_3.compute_googleapis_com_vpc_flows_20210920 was not found in location US\n",
      "404 POST https://bigquery.googleapis.com/bigquery/v2/projects/dspj-315716/jobs?prettyPrint=false: Not found: Dataset dspj-315716:experiment_syncmesh_with_latency_6\n",
      "404 POST https://bigquery.googleapis.com/bigquery/v2/projects/dspj-315716/jobs?prettyPrint=false: Not found: Dataset dspj-315716:experiment_baseline_with_latency_6\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# PCAPs"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "def download_pcap(ip, outDir = datetime.datetime.today().strftime('%Y%m%d-%H')):\n",
    "    ensureDirectory(outDir)\n",
    "    print(\"Dowloading File\")\n",
    "    print(os.path.join(Path.cwd(), \"..\", \"cert\"))\n",
    "    result = subprocess.run(f\"scp -i ../infrastructure/orchestrator.pem -o StrictHostKeyChecking=no orchestrator@{ip}:/captures.zip ./captures.zip\", shell=True, stdout=PIPE, stderr=PIPE, universal_newlines=True)\n",
    "    if(result.returncode == 0):\n",
    "        print(result.stdout)\n",
    "    else:\n",
    "        raise Exception(result.stderr)\n",
    "    shutil.unpack_archive(\"./captures.zip\", outDir)\n",
    "\n",
    "if 'experiment' not in locals():\n",
    "    download_pcap(\"35.224.133.98\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Monitoring"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    '../infrastructure/credentials.json')\n",
    "\n",
    "# Our project ID\n",
    "\n",
    "monitoring_client = monitoring_v3.MetricServiceClient(credentials=credentials)\n",
    "project_name = f\"projects/{project_id}\"\n",
    "now = time.time()\n",
    "seconds = int(now)\n",
    "nanos = int((now - seconds) * 10 ** 9)\n",
    "\n",
    "# 3600 = Get the last hour of metrics\n",
    "timeframe = 3600 * 1\n",
    "if 'experiment' not in locals():\n",
    "    print(\"set timeframe to one hour\")\n",
    "    timeframe = 3600 * 1\n",
    "\n",
    "interval = monitoring_v3.TimeInterval(\n",
    "    {\n",
    "        \"end_time\": {\"seconds\": seconds, \"nanos\": nanos},\n",
    "        \"start_time\": {\"seconds\": (seconds - timeframe), \"nanos\": nanos},\n",
    "    }\n",
    ")\n",
    "\n",
    "# Add Filter? metric.label.instance_name = \"your-instance-id\"\n",
    "results_cpu = monitoring_client.list_time_series(\n",
    "    request={\n",
    "        \"name\": project_name,\n",
    "        \"filter\": 'metric.type =  \"compute.googleapis.com/instance/cpu/utilization\"',\n",
    "        \"interval\": interval,\n",
    "        \"view\": monitoring_v3.ListTimeSeriesRequest.TimeSeriesView.FULL,\n",
    "    }\n",
    ")\n",
    "results_io_read = monitoring_client.list_time_series(\n",
    "    request={\n",
    "        \"name\": project_name,\n",
    "        \"filter\": 'metric.type = \"compute.googleapis.com/instance/disk/read_bytes_count\"',\n",
    "        \"interval\": interval,\n",
    "        \"view\": monitoring_v3.ListTimeSeriesRequest.TimeSeriesView.FULL,\n",
    "    }\n",
    ")\n",
    "results_io_write = monitoring_client.list_time_series(\n",
    "    request={\n",
    "        \"name\": project_name,\n",
    "        \"filter\": 'metric.type = \"compute.googleapis.com/instance/disk/write_bytes_count\"',\n",
    "        \"interval\": interval,\n",
    "        \"view\": monitoring_v3.ListTimeSeriesRequest.TimeSeriesView.FULL,\n",
    "    }\n",
    ")\n",
    "results_iops_read = monitoring_client.list_time_series(\n",
    "    request={\n",
    "        \"name\": project_name,\n",
    "        \"filter\": 'metric.type = \"compute.googleapis.com/instance/disk/read_ops_count\"',\n",
    "        \"interval\": interval,\n",
    "        \"view\": monitoring_v3.ListTimeSeriesRequest.TimeSeriesView.FULL,\n",
    "    }\n",
    ")\n",
    "results_iops_write = monitoring_client.list_time_series(\n",
    "    request={\n",
    "        \"name\": project_name,\n",
    "        \"filter\": 'metric.type = \"compute.googleapis.com/instance/disk/write_ops_count\"',\n",
    "        \"interval\": interval,\n",
    "        \"view\": monitoring_v3.ListTimeSeriesRequest.TimeSeriesView.FULL,\n",
    "    }\n",
    ")\n",
    "\n",
    "for item in results_cpu:\n",
    "    print(item.metric.labels[\"instance_name\"])\n",
    "\n",
    "# Prepare Data so that as the monitoring time_series_list is not aligned\n",
    "time_series_list_list_labels = [\"cpu_util_\", \"io_read_\", \"io_write_\", \"iops_read_\", \"iops_write_\"]\n",
    "time_series_list_list = [results_cpu, results_io_read, results_io_write, results_iops_read, results_iops_write]\n",
    "instance_logs = {}\n",
    "\n",
    "for index, time_series_list in enumerate(time_series_list_list):\n",
    "    for time_series in time_series_list:\n",
    "        label = time_series.metric.labels[\"instance_name\"]\n",
    "        if label not in instance_logs:\n",
    "            instance_logs[label] = np.empty(len(time_series_list_list), dtype=object)\n",
    "        instance_logs[label][index] = time_series\n",
    "\n",
    "def loadMonitoringData(experiment, outDir = datetime.datetime.today().strftime('%Y%m%d-%H')): \n",
    "    ensureDirectory(outDir)\n",
    "    df = pd.DataFrame()\n",
    "    set_timestamp_column = True\n",
    "    first_len = 0\n",
    "    # Filter instances for our experiment\n",
    "    filteredInstances = {}\n",
    "    for key, instance in instance_logs.items():\n",
    "        if key.startswith(experiment):\n",
    "            filteredInstances[key] = instance\n",
    "\n",
    "    # Find out which point array is smallest\n",
    "    first_len = min([len(item.points) for instances in filteredInstances.values() for item in instances])\n",
    "    for instances in filteredInstances.values():\n",
    "        print(instances[0].metric.labels[\"instance_name\"], len(instances[0].points))\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Overall min len:\", first_len)\n",
    "\n",
    "    for key, instances in filteredInstances.items():\n",
    "        for index in range(len(instances)):\n",
    "            label = key\n",
    "            # if \"orchestrator\" in label:\n",
    "            #     # Skip Orchesrtator\n",
    "            #     continue\n",
    "            if set_timestamp_column:\n",
    "                # first_len = min([len(item.points) - 1 for item in instances])\n",
    "                # print(first_len)\n",
    "                print(\"SET min len:\", first_len)\n",
    "\n",
    "                df['timestamp'] = pd.to_datetime([p.interval.start_time.ToDatetime() for p in instances[0].points[:first_len]])\n",
    "                set_timestamp_column = False\n",
    "            # print(ts.points[0])\n",
    "            # When deploying the vm they might take different amount of time leading to some values beeing available a minute early this leading to different length\n",
    "            # We can trim off the last values as they are orderer from most recent to last\n",
    "            df[time_series_list_list_labels[index] + label] = [p.value.double_value for p in instances[index].points[:first_len]]\n",
    "\n",
    "            # df['cpu_util_' + label] = [p.value.double_value for p in ts_cpu.points[:first_len]]\n",
    "            # df['io_read_' + label] = [p.value.int64_value for p in ts_io_read.points[:first_len]]\n",
    "            # df['io_write_' + label] = [p.value.int64_value for p in ts_io_write.points[:first_len]]\n",
    "            # df['iops_read_' + label] = [p.value.int64_value for p in ts_iops_read.points[:first_len]]\n",
    "            # df['iops_write_' + label] = [p.value.int64_value for p in ts_iops_write.points[:first_len]]\n",
    "\n",
    "    df.set_index('timestamp', inplace=True)\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    columns = df.columns\n",
    "    df.to_csv(f\"{outDir}/monitoring.csv\")\n",
    "    print(f\"Gathered all Monitoring data for {experiment}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# Make sure your experiment name is included in the experiments list, otherwise only errors will occur.\n",
    "if 'experiment' not in locals():\n",
    "    for experiment in sharedVariables.experiments: \n",
    "        try: \n",
    "            loadMonitoringData(experiment)\n",
    "        except Exception as e: \n",
    "            print(e)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "experiment-baseline-with-latency-3-node-instance-2\n",
      "experiment-syncmesh-with-latency-3-node-instance-2\n",
      "experiment-advanced-mongo-with-latency-3-node-instance-2\n",
      "experiment-baseline-with-latency-3-node-instance-2\n",
      "experiment-baseline-with-latency-3-node-instance-2\n",
      "experiment-baseline-with-latency-3-node-instance-3\n",
      "experiment-advanced-mongo-with-latency-3-node-instance-3\n",
      "experiment-baseline-with-latency-3-node-instance-3\n",
      "experiment-baseline-with-latency-3-node-instance-3\n",
      "experiment-syncmesh-with-latency-3-node-instance-3\n",
      "experiment-syncmesh-with-latency-3-node-instance-1\n",
      "experiment-advanced-mongo-with-latency-3-node-instance-1\n",
      "experiment-baseline-with-latency-3-node-instance-1\n",
      "experiment-baseline-with-latency-3-node-instance-1\n",
      "experiment-baseline-with-latency-3-node-instance-1\n",
      "experiment-baseline-with-latency-3-central-server\n",
      "experiment-baseline-with-latency-3-client-instance\n",
      "experiment-baseline-with-latency-3-test-orchestrator\n",
      "experiment-baseline-with-latency-3-central-server\n",
      "experiment-advanced-mongo-with-latency-3-central-server\n",
      "experiment-advanced-mongo-with-latency-3-client-instance\n",
      "experiment-syncmesh-with-latency-3-test-orchestrator\n",
      "experiment-advanced-mongo-with-latency-3-test-orchestrator\n",
      "experiment-syncmesh-with-latency-3-client-instance\n",
      "experiment-advanced-mongo-with-latency-3-central-config-server\n",
      "experiment-baseline-with-latency-3-client-instance\n",
      "experiment-baseline-with-latency-3-test-orchestrator\n",
      "experiment-baseline-with-latency-3-node-instance-2 61\n",
      "experiment-baseline-with-latency-3-node-instance-3 61\n",
      "experiment-baseline-with-latency-3-node-instance-1 61\n",
      "experiment-baseline-with-latency-3-central-server 26\n",
      "experiment-baseline-with-latency-3-client-instance 60\n",
      "experiment-baseline-with-latency-3-test-orchestrator 25\n",
      "Overall min len: 25\n",
      "SET min len: 25\n",
      "Gathered all Monitoring data for experiment-baseline-with-latency-3\n",
      "experiment-syncmesh-with-latency-3-node-instance-2 64\n",
      "experiment-syncmesh-with-latency-3-node-instance-3 63\n",
      "experiment-syncmesh-with-latency-3-node-instance-1 64\n",
      "experiment-syncmesh-with-latency-3-test-orchestrator 61\n",
      "experiment-syncmesh-with-latency-3-client-instance 60\n",
      "Overall min len: 60\n",
      "SET min len: 60\n",
      "Gathered all Monitoring data for experiment-syncmesh-with-latency-3\n",
      "experiment-advanced-mongo-with-latency-3-node-instance-2 62\n",
      "experiment-advanced-mongo-with-latency-3-node-instance-3 61\n",
      "experiment-advanced-mongo-with-latency-3-node-instance-1 63\n",
      "experiment-advanced-mongo-with-latency-3-central-server 60\n",
      "experiment-advanced-mongo-with-latency-3-client-instance 60\n",
      "experiment-advanced-mongo-with-latency-3-test-orchestrator 59\n",
      "experiment-advanced-mongo-with-latency-3-central-config-server 59\n",
      "Overall min len: 59\n",
      "SET min len: 59\n",
      "Gathered all Monitoring data for experiment-advanced-mongo-with-latency-3\n",
      "min() arg is an empty sequence\n",
      "min() arg is an empty sequence\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Get TimeStamps"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "# # Workaround for now\n",
    "# # df_sync3 = pd.read_csv(f'{outdir}/experiment-syncmesh-with-latency-3.csv')\n",
    "# # df_base3 = pd.read_csv(f'{outdir}/experiment-baseline-with-latency-3.csv')\n",
    "# df_sync3 = loadData(f'{outdir}/experiment-syncmesh-with-latency-3.log')\n",
    "# df_base3 = loadData(f'{outdir}/experiment-baseline-with-latency-3.log')\n",
    "# # df_base3.set_index('timestamp', inplace=True)\n",
    "# # df_sync3.set_index('timestamp', inplace=True)\n",
    "\n",
    "# # df_sync6 = loadData(f'{outdir}/experiment-syncmesh-with-latency-6.log')\n",
    "# # df_base6 = loadData(f'{outdir}/experiment-baseline-with-latency-6.log')\n",
    "\n",
    "# def filterDataForSeperator(df): \n",
    "#     df = df[df[\"jsonPayload.connection.src_ip\"].isin([ip_seperator, ip_orchestrator])]\n",
    "#     df = df[df[\"jsonPayload.connection.dest_ip\"].isin([ip_seperator, ip_orchestrator])]\n",
    "#     df = df[df[\"jsonPayload.connection.dest_port\"] == 443]\n",
    "#     return df\n",
    "\n",
    "# seperator_base3 = filterDataForSeperator(df_base3)\n",
    "# seperator_sync3 = filterDataForSeperator(df_sync3)\n",
    "\n",
    "# # df = df[df[\"jsonPayload.connection.dest_ip\"]]\n",
    "# print(seperator_sync3.index)\n",
    "# print(seperator_base3.index)\n",
    "# # df_base3.info()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "# Script for automatic data retrieval via papermill\n",
    "if 'experiment' in locals():\n",
    "    todaystring = datetime.datetime.today().strftime('%Y%m%d-%H')\n",
    "    outdir = f\"data/{todaystring}-{experiment}\"\n",
    "\n",
    "    f = open(os.path.join(Path.cwd(), \"..\", \"infrastructure\", \"orchestrator.txt\"), \"r\")\n",
    "    ip = f.read()\n",
    "    download_pcap(ip, outDir=outdir)\n",
    "    loadMonitoringData(experiment, outDir=outdir)\n",
    "    # getDataset(experiment, outDir=outdir)\n",
    "\n",
    "    "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dowloading File\n",
      "c:\\Develop\\GitHub\\DSPJ2021\\syncmesh\\evaluation\\..\\cert\n",
      "\n",
      "experiment-advanced-mongo-with-latency-3-node-instance-2: 165\n",
      "164\n",
      "experiment-advanced-mongo-with-latency-3-node-instance-2: 36\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Length of values (36) does not match length of index (164)",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24508/4231757217.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mip\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mdownload_pcap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mip\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutDir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutdir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mloadMonitoringData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexperiment\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutDir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutdir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mgetDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexperiment\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutDir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutdir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24508/904242325.py\u001b[0m in \u001b[0;36mloadMonitoringData\u001b[1;34m(experiment, outDir)\u001b[0m\n\u001b[0;32m     81\u001b[0m             \u001b[1;31m# When deploying the vm they might take different amount of time leading to some values beeing available a minute early this leading to different length\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m             \u001b[1;31m# We can trim off the last values as they are orderer from most recent to last\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m             \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'cpu_util_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdouble_value\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mts_cpu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpoints\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mfirst_len\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m             \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'io_read_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint64_value\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mts_io_read\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpoints\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mfirst_len\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m             \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'io_write_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint64_value\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mts_io_write\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpoints\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mfirst_len\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3605\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3606\u001b[0m             \u001b[1;31m# set column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3607\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3608\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3609\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3777\u001b[0m         \u001b[0mensure\u001b[0m \u001b[0mhomogeneity\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3778\u001b[0m         \"\"\"\n\u001b[1;32m-> 3779\u001b[1;33m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3780\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3781\u001b[0m         if (\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m   4502\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4503\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4504\u001b[1;33m             \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequire_length_match\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4505\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0msanitize_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4506\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\common.py\u001b[0m in \u001b[0;36mrequire_length_match\u001b[1;34m(data, index)\u001b[0m\n\u001b[0;32m    529\u001b[0m     \"\"\"\n\u001b[0;32m    530\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 531\u001b[1;33m         raise ValueError(\n\u001b[0m\u001b[0;32m    532\u001b[0m             \u001b[1;34m\"Length of values \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m             \u001b[1;34mf\"({len(data)}) \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Length of values (36) does not match length of index (164)"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}